# ASMUS Workshop '23

<div align=center>
 <img src="im/asmus.png" height=150px>
</div>

> **The ASMUS Workshop will be held on October 8th at 8 a.m. in Meeting Room 8 at Level 1. You can also [join virtually](https://miccai2023.conflux.events/app/schedule/session/3317). See you there!**
> 

**The 4th International Workshop of Advances in Simplifying Medical UltraSound (ASMUS) - a workshop held in conjunction with [MICCAI 2023](https://conferences.miccai.org/2023/), the 26th International Conference on Medical Image Computing and Computer Assisted Intervention.**

**ASMUS is the official workshop of the [MICCAI Special Interest Group on Medical Ultrasound](home).**

> **The workshop proceedings will be published before the conference starts [https://link.springer.com/book/10.1007/978-3-031-44521-7](https://link.springer.com/book/10.1007/978-3-031-44521-7). Workshop attendees can access the proceedings until the four weeks after the conference.**


## Call for Papers

Papers will consist of a maximum 8 pages (text, figures, and tables) + up to 2 pages for references only. They are to be submitted electronically in [Springer LNCS (Lecture Notes in Computer Science) style](https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines) and are subject to double-blind review. Information on submission to follow.

The papers will be evaluated by external reviewers and our organizing committee for inclusion in the workshop as a presentation (oral or poster). Accepted full-length manuscripts will be published with Springer LNCS, and the best papers will be selected for industry-sponsored awards. Original research contributions are invited. Proof-of-concept research from novel research directions is also encouraged.

One of the popular features of ASMUS, live demonstrations, will return for ASMUS 2023. Capitalising on the unique real-time and portability aspects of ultrasound-based applications, we plan for live demonstrations covering AI, interventional, and robotics areas. All accepted papers will be offered the option to present a live demonstration.

You may start the submission process at [Springer Nature EquinOCS](https://equinocs.springernature.com/service/ASMUS2023). E-mail registration is required prior to submission, which can be accessed by clicking the "Submit now" button on the previous link. If you have any issues registering or with the submission process, please contact the PC Chairs via the platform or via E-Mail.

In this exciting era for medical ultrasound, recent developments in deep learning (artificial intelligence) and medical robotics have started to show clinically measurable improvement in assisting ultrasound examinations, ultrasound-guided interventions, and surgery. This year, ASMUS is soliciting submissions, including work from the following areas:

### Ultrasound Assisted by Artificial Intelligence and Medical Robotics:
- Ultrasound imaging with robotic (automated) assistance
- Machine learning methods in ultrasound analysis and guidance
- Automated interpretation and measurement for ultrasound
- Ultrasound quality and skills assessment

### Multimodality Ultrasound Imaging:
- Ultrasound with other non-imaging sensory information, e.g. positional and eye tracking
- Ultrasound with another pre-/intra-procedural imaging, e.g. camera videos, CT, MR, fluorescence
- Different modes of ultrasound imaging, e.g. photoacoustic, Doppler, functional ultrasound, tissue quantification

### Applications:
- Global healthcare
- Training sonographers and other users
- Assisting non-expert healthcare professionals
- Point-of-care ultrasound systems and scenarios
- Assisting surgery and interventions
- Streamlining clinical ultrasound workflow
- Sonography data science


| Workshop Timeline             |                            |
| ----------------------------- | -------------------------- |
| ~~July 3 2023~~ July 10 2023  | Paper Submission Deadline  |
| ~~July 28 2023~~ | Reviews Due |
| ~~July 28 2023~~ ~~August 3 2023~~ | Notification of Acceptance |
| ~~August 18 2023~~        | Camera Ready Submission    |
| October 8 2023                | ASMUS Workshop             |

All times are in "anywhere on earth". 

## Program

**The half-day ASMUS workshop will take place on October 8th 2023, and include live practical technology demonstrations, paper presentations, Q&A sessions, and keynote talks.**

### Program

07:50-08:00 Introduction
 
08:00 - 09:00 (12 + 3 min): *Segmentation, predictions, and automated assessments* (Bernhard Kainz, Alberto Gomez)
- paper 1: Zhao, Men, Gleed, Papageorghiou, Noble: Ultrasound Video Segmentation with Adaptive Temporal Memory
- paper 2: Pegios, Pi Fogtmann Sejer, Lin, Bashir, Bo Søndergaard Svendsen, Nielsen, Petersen, Nymark Christensen, Tolsgaard, Feragen: Leveraging Shape and Spatial Information for Spontaneous Preterm Birth Prediction 
- paper 3: Wong, Lin, Raheli, Bashir, Svendsen, Tolsgaard, Feragen, Christensen: An Automatic Guidance and Quality Assessment System for Doppler Imaging of Umbilical Artery
- paper 4: Benjamin, Asokan, Alhosani, Alasmawi, Diehl, Bricker, Nandakumar, Yaqub: Leveraging Self-Supervised Learning for Fetal Cardiac Planes Classification using Ultrasound Scan Videos

09:00 - 10:00 *Poster lightning talks (5 min)* (Bernhard Kainz, Alberto Gomez)
- paper l1: Singla, Ringstrom, Hu, Lessoway, Reid, Nguan, Rohlign: The Open Kidney Ultrasound Data Set
- paper l2: Ng, Gao, Mohammed Furqan, Yeo, Lau, Ngiam, Khoo: HoloPOCUS: Mixed Reality 3D Ultrasound Reconstruction and Overlay
- paper l3: Bhattacharya, Vesal, Jahanandish, Choi, Zhou, Kornberg, Sommer, Fan, Brooks, Sonn, Rusu: MIC-CUSP: Multimodal Image Correlations for Ultrasound-based Prostate Cancer Detection
- paper l4: Colussi, Mascetti, Ahmetovic, Civitarese, Cacciatori, Peyvandi, Gualtierotti, Arcudi, Bettini: GAJA - Guided self-Acquisition of Joint ultrAsound images
- paper l5: Li, Shen, Li, Barratt, Dowrick, Clarkson, Vercauteren, Hu: Privileged Anatomical and Protocol Discrimination in Trackerless 3D Ultrasound Reconstruction
- paper l6: Stojanovski, Hermida, Lamata, Beqiri, Gomez: Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation
- paper l7: FANG, Delingette, Ayache: Anatomical Landmark Detection for Initializing US and MR Image Registration
- paper l8: Thomas, Tiago , Andreassen, Aase, Sprem, Steen, Solberg, Ben-Yosef: Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach
- paper l9: Zhou, Knight, Felfeliyan, Gosh, Alves-Pereira, Keen, Hareendranathan, Jaremko: Self-supervised learning to more efficiently generate segmentation masks for wrist ultrasound
- paper l10: Adhikari, Dhakal, Thapaliya, Bhandari, Poudel, Khanal: Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography
- paper l11: Gonzalez Duque, Zirus, Velikova, Navab, Mateus: Can ultrasound confidence maps predict sonogaphers labeling variability?

10:00 - 10:30: *Coffee break, live demos, and poster session* (Matthew Baugh, Johanna Müller)

*Demos:*
- Connolly, Deguet,  Ungi, Kumar, Lasso, Sunderland, Kazanzides, Krieger, Tokuda, Leonard, Fichtinger, Mousavi, and Taylor: [Haptic feedback and ultrasound guidance for
lumpectomy with SlicerROS2](https://miccai-ultrasound.github.io/files/Connolly_MICCAI_ASMUS_demo_v1.pdf)
- Ng, Gao, Mohammed Furqan, Yeo, Lau, Ngiam, Khoo: HoloPOCUS: Portable Mixed-Reality 3D Ultrasound Tracking, Reconstruction and Overlay
- Ultromics: [EchoGo Precision HFpEF detection](https://www.ultromics.com/products/echogo-heart-failure)
- ThinkSono: [DVT detection at the front line of care](https://thinksono.com/)
- ImFusion: [ImFusion Suite for Ultrsound image analysis](https://www.imfusion.com/products/imfusion-suite)

10:30 - 11:30: *Keynote address by Prof. Mirabela Rusu* (Alison Noble, Purang Abolmaesumi)

11:30 - 12:30 (12 + 3 min) *Contrastive Learning, and Multi-Task Innovations* (Alison Noble, Purang Abolmaesumi)
- paper 5: Ravishankar, Annangi, Melapudi, Patil, Patil: SonoSAM - Segment Anything on Ultrasound Images
- paper 6: Prieto, Benabdelkader, Pokaprakarn, Shah, Sebastião, Dan, Almnini, Diaz, Chari, Chi, Stringer, Stringer: SimNorth: A novel contrastive learning approach for clustering prenatal ultrasound images 
- paper 7: Charton, Ren, Kim, Gonzalez, Khambhati, Cheng, DeFrancesco, Waheed, Marciniak, Moura, Cardoso, Lima, Picard, Li, Li: Multi-task Learning for Hierarchically-Structured Images: Study on Echocardiogram View Classification 
- paper 8: Tafuro, Jansen, Išgum: Temporally consistent segmentations from sparsely labeled echocardiograms using image registration for pseudo-labels generation

12:30 - 12:50 *Closing remarks, challenge, and prizes* 

Lunch at 12:30-13:30

## Keynote Speaker

### Prof. Mirabela Rusu
![Prof. Mirabela Rusu](https://profiles.stanford.edu/proxy/api/cap/profiles/185796/resources/profilephoto/350x350.1513908263182.jpg)

*Title:* Artificial intelligence methods for b-mode ultrasound of the prostate to guide the biopsy procedure: Are we there yet? 

*Abstract:* Despite the advancements in prostate MRI, most prostate biopsies are still performed under the guidance of ultrasound alone. These procedures are imperfect, missing >50% of clinically significant cancers. By comparison, when MRI is utilized in an MRI-Ultrasound guided biopsy, the procedure only misses 12% of cancers. My team’s research has been focusing on using the ubiquitous but noisy b-mode ultrasound images to detect and localize prostate cancer, in approaches that often rely on MRI during the training of the models, but only use b-mode ultrasound images at inference time in new patients. Improving the ability of ultrasound to better show cancer has potential to enable early cancer detection, sparing men of biopsies when they are not needed, and allow for better targeting during local treatment. 

*Bio:* Dr. Mirabela Rusu received her MS and PhD in Computational Biomedicine from University of Texas, Houston, and focused her research on the fusion of biomolecular structural data from different sources (i.e., cryo-electron microscopy and X-ray crystallography). Her postdoctoral training at Rutgers and Case Western Reserve University was focused on developing computational methods for the fusion of medical images, i.e., to register radiology or pathology images, or create population atlases for prostate cancer studies. Following postdoctoral training, Dr. Rusu joined Industry as an Image Analysis Scientist/Lead Engineer. Currently, Dr. Rusu is an Assistant Professor of Radiology, and by courtesy, Urology and Data Science, at Stanford University, where she leads the Personalized Integrative Medicine Laboratory (http://pimed.stanford.edu). Dr. Rusu’s team focuses on developing analytic methods to improve the interpretation of radiology images by taking advantage of existing high-resolution information during training but only needing lower-resolution radiology images during inference (e.g., when applied in new patients).  

## Challenge 

### [µ-RegPro Challenge](https://muregpro.github.io/)
Multimodal image registration between pre-operative and intra-operative imaging enables the fusion of clinically important information during many surgical and interventional tasks. The registration of magnetic resonance imaging (MR) and transrectal ultrasound (TRUS) images assists prostate biopsy and focal therapy, arguably having transformed prostate cancer patient care to a less invasive and more localized diagnostic, monitoring and treatment pathway. Though, even with great progress having been made by the community in the past two decades, challenges remain in this application. 
The µ-RegPro challenge aims to provide well-curated, yet real-world clinical data, with more than a hundred paired MR and TRUS images, annotated carefully by researchers and clinicians with more than 15 years of experience working with this application. The outcome of the challenge includes one of the first multimodal imaging datasets, facilitated with expert annotations for validation, for benchmarking advancement in registration methodologies, as well as for future research in managing the most common non-skin cancer in men.
The outcome of this challenge is the development of a method which produces an accurate alignment of the prostate gland and other anatomical structures, such as those provided as landmarks for localizing relevant anatomical and potentially pathological targets during guided prostate biopsies, and for treatment/intervention planning or decision support. 
The challenge event will take place before lunch (12:30 - 12:50) during the ASMUS workshop. 


## Organizers
### Chairs
* Bernhard Kainz (Co-Chair, FAU Erlangen-Nürnberg, DE, and Imperial College London, UK)
* Julia Schnabel (Co-Chair, Technical University of Munich, DE)
* Bishesh Khanal (outreach Co-Chair, NAAMII, Nepal)

### Organising Committee
* Alison Noble (University of Oxford, UK)
* Stephen Aylward (Kitware, US)
* Yipeng Hu (University College London, UK)
* Purang Abolmaesumi (University of British Columbia, CA)
* Dong Ni (Shenzhen University, CN)
* Emad Boctor (Johns Hopkins University, US)
* Andy King (King’s College London, UK)
* Ana Namburete (University of Oxford, UK)
* Thomas van den Heuvel (Radboud University, NL)
* Wolfgang Wein (ImFusion, DE)
* Parvin Mousavi (Queen’s University, CA)
* Alberto Gomez (King’s College London and Ultromics, UK)
* Veronika Zimmer (Technical University of Munich, DE)

### Delivery Team
* Thomas Day (King’s College London, UK)
* Mischa Dombrowski (FAU Erlangen-Nürnberg, DE)
* Johanna Mueller (FAU Erlangen-Nürnberg, DE)
* Matthew Baugh (Imperial College London, UK)
* Zachary Baum (University College London, UK)

### Advisory Board

* Gabor Fichtinger (Queen’s University, CA)
* Kawal Rhode (King’s College London, UK)
* Russ Taylor (Johns Hopkins University, USA)
* Chris de Korte (Radboud University Nijmegen, NL)
* Nassir Navab (Technical University of Munich, DE)
* Reza Razavi (King’s College London, UK)
* Joseph V. Hajnal (King’s College London, UK)
